{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nepslor/B5203E-TSAF/blob/main/W4/fourier_forecasts_and_regression_pt2.ipynb)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fourier decomposition and forecasting\n",
    "Under stationarity assumption, Fourier decomposition can be used to extrapolate a signal beyond observed values. In this lesson you'll write a simple Fourier forecaster from scratch. Let's start downloading some time series\n"
   ],
   "metadata": {
    "id": "i-TBEtoa3EGy"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mq6X7MpY414Y",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "outputId": "9ef34f4b-b4a9-47b5-a471-10e51bfe9954"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_power = pd.read_pickle('https://github.com/nepslor/teaching/raw/refs/heads/main/TimeSeriesForecasting/data/power_dataset.pk')\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "df_power.filter(like='meter').plot(alpha=0.4, linewidth=1, legend=False, figsize=(15, 3))\n",
    "df_power[['power', 'temperature', 'irradiance']].plot(linewidth=1, subplots=True, figsize=(15, 3))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ❓ Generate the basis functions\n",
    "Complete the following code that creates the first $n_h$ basis functions defined as:\n",
    "$$P = \\left[sin\\left(\\frac{2\\pi tk}{l}\\right), cos\\left(\\frac{2\\pi tk}{l}\\right)\\right] \\quad k\\in \\{1,\\dots n_h\\}$$\n",
    "where $l$ is the length of $t \\in \\mathbb{N}^l$.\n",
    "\n",
    "We then see that the cross-covariance matrix of these basis functions is normal, that is, off-diagonal elements are 0.\n",
    "\n",
    "Remember that we have to normalize the signals to obtain ortho-**normal** basis. When we integrated the trigonometric functions over 2$\\pi$, we shown tha the normalization constant was $\\sqrt{\\pi}$. Since we're now integrating over $l$, the normalization constant in this case is $\\sqrt{l/2}$, that is the square root of half of the integration period:\n",
    "$$ P_n = \\sqrt{2/l} \\left[sin\\left(\\frac{2\\pi tk}{l}\\right), cos\\left(\\frac{2\\pi tk}{l}\\right)\\right] \\quad k\\in \\{1,\\dots n_h\\}$$"
   ],
   "metadata": {
    "id": "fQIX65rZ6UeO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "l=100               # lenght of the signal\n",
    "n_h = 3             # number of harmonics to plot\n",
    "t = np.arange(l)    # time vector\n",
    "\n",
    "def get_basis(t, l, n_h):\n",
    "  \"\"\"\n",
    "  Get the first n_h sine and cosine basis functions and the projection\n",
    "  matrix P\n",
    "  \"\"\"\n",
    "  sines = np.vstack([np.sin(2*np.pi*k*t/l) for k in np.arange(n_h)+1])\n",
    "  cosines = np.vstack([np.cos(2*np.pi*k*t/l) for k in np.arange(n_h)+1])\n",
    "  P = None # ? COMPLETE THIS LINE\n",
    "  return P\n",
    "\n",
    "P = get_basis(t, l, n_h)\n",
    "sines, cosines = P[:, :n_h].T, P[:, n_h:].T\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(15, 3), layout=\"tight\")\n",
    "ax[0].plot(sines.T)\n",
    "ax[0].set_xticklabels([])\n",
    "ax[1].plot(cosines.T)\n",
    "ax[0].set_title('Sines and cosines basis')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 3))\n",
    "m = ax.matshow(P.T@P)\n",
    "plt.colorbar(m)\n",
    "\n"
   ],
   "metadata": {
    "id": "6UKgaKPWE6UX",
    "outputId": "0286dde6-6299-445d-eaaa-b072b8d4b9eb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is just an utility function to remove the mean from the power signal and return a training and a test set."
   ],
   "metadata": {
    "id": "Vr2i_mTs9d6Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_tr_te(p, training_steps, steps_ahead):\n",
    "  x_tr = p.iloc[:training_steps].values\n",
    "  tr_mean = np.mean(x_tr)\n",
    "  x_tr -= tr_mean\n",
    "  x_te = p.iloc[training_steps:training_steps+steps_ahead].values\n",
    "  x_te -= tr_mean\n",
    "  return x_tr, x_te\n",
    "\n",
    "steps_ahead = 24*14\n",
    "training_steps = 24*7\n",
    "\n",
    "x_tr, x_te = get_tr_te(df_power['power'], training_steps, steps_ahead)"
   ],
   "metadata": {
    "id": "W-_gzgop9Z7o"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ❓ Write a Fourier forecaster\n",
    "Once you have created the basis function with the same length of the training signal, you have to project the original signal onto the basis. Since the basis matrix is orthonormal:\n",
    "$$\\beta = (P^TP)^{-1}(P^Tx) =I(P^Tx)=\n",
    " P^Tx$$\n",
    "\n",
    "In order to extrapolate you just have to create a longer basis matrix $P$ and obtain the extrapolated signal using $\\beta$.\n",
    "\n",
    "You can reuse the `get_basis` function to define a new matrix, $P_{fore}$, defined over $l + l_{fore}$ where $l_{fore}$ is the length of the step ahead to be forecasted.\n",
    "\n"
   ],
   "metadata": {
    "id": "DHbMYzQF9Ige"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "l_tr=len(x_tr)\n",
    "l_te=len(x_te)\n",
    "t_tr = np.arange(l_tr)\n",
    "t_te = np.arange(l_tr, l_tr+l_te)\n",
    "n_harmonics = 20\n",
    "\n",
    "# ------------- Fourier decomposition - linear regression fit -----------------\n",
    "P = get_basis(t_tr, l_tr, n_harmonics)\n",
    "\n",
    "# since the basis is orthonormal the following is equivalent to the standard linear regression solution\n",
    "coeffs = None # ? COMPLETE THIS LINE\n",
    "\n",
    "# ------------- Extrapolation - we fix the frequencies and retrieve longer basis\n",
    "\n",
    "P_pred = None # ? COMPLETE THIS LINE\n",
    "\n",
    "\n",
    "\n",
    "plt.subplots(1, 1, figsize=(20, 4))\n",
    "plt.plot(x_tr, label='training data')\n",
    "plt.plot(np.hstack([x_tr*np.nan, x_te]), color='grey', linewidth=1, label='test data')\n",
    "plt.plot(t_te, P_pred@coeffs, label='extrapolation')\n",
    "plt.legend();"
   ],
   "metadata": {
    "id": "w9HwgkTxF2wH",
    "outputId": "d0c47e4e-1ab5-4041-c843-0510f26dbb5f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ❓ What's the optimal $n_h$?\n",
    "Write a function to get the MAE on the test set, then write an optimizer to obtain the optimal value of $n_h$\n"
   ],
   "metadata": {
    "id": "z13uHQy0GWpT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "n_harmonics = np.arange(2, 130)\n",
    "mae= np.zeros(len(n_harmonics))\n",
    "mae_fun = lambda err: np.mean(np.abs(err))\n",
    "\n",
    "t0=time()\n",
    "for i, n_h in enumerate(n_harmonics):\n",
    "  P = get_basis(t_tr, l_tr, n_h)\n",
    "  coeffs = P.T@x_tr\n",
    "  P_pred = get_basis(t_te, l_tr, n_h)\n",
    "  x_hat_1 = P_pred@coeffs\n",
    "  mae[i] = mae_fun(x_hat_1-x_te)\n",
    "print('our method explored {} solutions in {}s'.format(len(n_harmonics), time()-t0))\n",
    "\n",
    "plt.subplots(1, 1, figsize=(10, 4))\n",
    "plt.plot(n_harmonics, np.array(mae), linewidth=3)\n",
    "plt.ylabel('MAE on test set')\n",
    "plt.xlabel('$n_h$')\n",
    "plt.legend();"
   ],
   "metadata": {
    "id": "2V81G6_InjYV",
    "outputId": "27b48829-c3d1-42ab-d471-6b103d463002",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fourier forecaster via Nixtla\n",
    "The Nixtlaverse is composed by our open-source libraries, designed to provide a comprehensive, cutting-edge toolkit for time series forecasting in python.\n",
    "\n",
    "In the following we'll see how a similar Fourier forecaster can be written using Nixtla\n"
   ],
   "metadata": {
    "id": "M8BCBAVtrEIw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install mlforecast\n",
    "!pip install utilsforecast\n",
    "from utilsforecast.feature_engineering import trend, fourier, pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlforecast import MLForecast\n",
    "from utilsforecast.feature_engineering import fourier, pipeline\n",
    "from functools import partial\n",
    "from utilsforecast.plotting import plot_series"
   ],
   "metadata": {
    "id": "M8E9lhybsbe9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first step is to convert our timeseries in a nixtla-compatible format: we need an `unique_id` column with a tag for the name of the timeseries, a 'ds' timestamp and the 'y' containing the actual values:\n",
    "\n",
    "| unique_id | ds         | y         |\n",
    "|-----------|------------|-----------|\n",
    "| id_00     | 2000-01-01 | 17.519167 |\n",
    "| id_00     | 2000-01-02 | 87.799695 |\n",
    "| id_00     | 2000-01-03 | 177.442975|\n",
    "| id_00     | 2000-01-04 | 232.704110|\n",
    "| id_00     | 2000-01-05 | 317.510474|"
   ],
   "metadata": {
    "id": "c6x7Nr0_rjhw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def convert_to_nixtla_ts(series):\n",
    "  \"\"\"\n",
    "  Transform a pandas series to a nixtla-compatible df\n",
    "  \"\"\"\n",
    "  name = series.name\n",
    "  series = series.reset_index(name='y')\n",
    "  df = pd.DataFrame(series.rename(columns={'index': 'ds'}))\n",
    "  df['unique_id'] = name\n",
    "  return df\n",
    "\n",
    "print('#'*40)\n",
    "print('original series')\n",
    "print('#'*40)\n",
    "\n",
    "print(df_power['power'].head())\n",
    "\n",
    "df_nixtla = convert_to_nixtla_ts(df_power['power'])\n",
    "print('#'*40)\n",
    "print('nixtla compatible df')\n",
    "print('#'*40)\n",
    "\n",
    "print(df_nixtla.head())\n"
   ],
   "metadata": {
    "id": "TCs-FoXJvO3i",
    "outputId": "435b6329-5b38-4bac-f1fe-a8bb5413b567",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Step 1: Define Fourier features\n",
    "features = [\n",
    "    partial(fourier, season_length=24, k=2),\n",
    "    partial(fourier, season_length=24*7, k=1)\n",
    "]\n",
    "\n",
    "# Step 2: Run the pipeline to create training and future features\n",
    "# df must contain columns: ['unique_id', 'ds', 'y']\n",
    "train_df, future_df = pipeline(\n",
    "    df_nixtla.iloc[-24*30:-24],\n",
    "    features=features,\n",
    "    freq='1h',\n",
    "    h= 24\n",
    "    )\n",
    "\n",
    "train_df[[c for c in train_df.columns if 'y' not in c]].plot(x='ds', figsize=(15, 3))\n",
    "\n",
    "sf = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq='1h',\n",
    "    lags=[1]\n",
    ")\n",
    "\n",
    "sf = sf.fit(train_df, static_features=[])\n",
    "\n",
    "predictions = sf.predict(h=24, X_df=future_df)\n",
    "plot_series(df_nixtla.tail(24*7), predictions)"
   ],
   "metadata": {
    "id": "nJWvL-stCX3Z",
    "outputId": "a072a911-34fd-4579-dcce-9e20559c5bd4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 673
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Interactive Fourier model\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "\n",
    "def fourier_interactive(df_nixtla, p_1 = 24, p_2 = 24 * 7):\n",
    "  # Create dropdown to toggle order\n",
    "  k1_slider = widgets.IntSlider(value=1, min=1, max=50, step=1, description='k1 (daily)', continuous_update=False)\n",
    "  k2_slider = widgets.IntSlider(value=1, min=1, max=50, step=1, description='k2 (weekly)', continuous_update=False)\n",
    "\n",
    "\n",
    "  def update_plot(k1, k2):\n",
    "\n",
    "        features = [\n",
    "            partial(fourier, season_length=p_2, k=k2),       # weekly\n",
    "            partial(fourier, season_length=p_1, k=k1),         # daily\n",
    "        ]\n",
    "        # Step 2: Run the pipeline to create training and future features\n",
    "        # df must contain columns: ['unique_id', 'ds', 'y']\n",
    "        train_df, future_df = pipeline(\n",
    "            df_nixtla.tail(p_1+p_2)[:-p_1],\n",
    "            features=features,\n",
    "            freq='1h',\n",
    "            h= p_1\n",
    "        )\n",
    "\n",
    "\n",
    "        sf = MLForecast(\n",
    "            models=[LinearRegression(fit_intercept=True)],\n",
    "            freq='1h',\n",
    "            lags=[1]\n",
    "        )\n",
    "\n",
    "        sf = sf.fit(train_df, static_features=[])\n",
    "\n",
    "        predictions = sf.predict(h=p_1, X_df=future_df)\n",
    "        predictions = predictions.rename(columns={'LinearRegression': 'y'})\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n",
    "        df_nixtla.tail(p_1 + p_2).plot(x='ds', y='y', ax=ax)\n",
    "        predictions.plot(x='ds', y='y', ax=ax)\n",
    "        # plot ground truth\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "  # Link widget to function\n",
    "  ui = widgets.VBox([k1_slider, k2_slider])\n",
    "  out = widgets.interactive_output(update_plot, {'k1': k1_slider, 'k2': k2_slider})\n",
    "\n",
    "  return display(ui, out)\n",
    "\n",
    "fourier_interactive(df_nixtla)"
   ],
   "metadata": {
    "id": "dYtV9VoGEMXj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression via embeddings - direct approach\n",
    "<img src=\"https://github.com/nepslor/B5203E-TSAF/raw/95ab6491476169ca761b47d1bee8735b12346694/pics/direct_forecast.png\" width=\"600\">\n",
    "\n",
    "We've seen how to craft a Fourier forecaster, by using sine and cosine basis functions defined over time. This is a linear regression problem, but at the end what's happening is that *we are copy-pasting a low fidelity (or smoothed) representation of the signal*.\n",
    "\n",
    "A more general approach is the linear regression via **embeddings**.\n",
    "\n",
    "Let's say we want to forecast $m_f$ steps in the future using the last $m_p$ lags. We can then define a dataset of delayed vectors.\n",
    "\\begin{align}\n",
    "E(m_p  + m_f) &= [Y_p, Y_f]\\\\\n",
    "=&\\begin{bmatrix}\n",
    "\\color{darkred}{y(0)}       & \\color{darkred}{y(1)}   & \\color{darkred}{\\cdots y(m_p)}    & \\color{darkblue}{y(m_p + 1)}       & \\color{darkblue}{\\cdots} & \\color{darkblue}{y(m_p+m_f)} \\\\\n",
    "\\color{darkred}{y(1)}       & \\color{darkred}{y(2)}   & \\color{darkred}{\\cdots y(m_p + 1)} & \\color{darkblue}{y(m_p + 2)}    & \\color{darkblue}{\\cdots} & \\color{darkblue}{y(m_p+m_f+1)} \\\\\n",
    "\\color{darkred}{y(2)}       & \\color{darkred}{y(3)}   & \\color{darkred}{\\cdots y(m_p + 2)} & \\color{darkblue}{y(m_p + 3)}    & \\color{darkblue}{\\cdots} & \\color{darkblue}{y(m_p+m_f+2)} \\\\\n",
    "\\color{darkred}{\\vdots}     & \\color{darkred}{\\vdots} & \\color{darkred}{\\vdots}            & \\color{darkblue}{\\vdots}         & \\color{darkblue}{\\ddots} & \\color{darkblue}{\\vdots} \\\\\n",
    "\\color{darkred}{y(T-m_p-m_f)} & \\color{darkred}{y(T-m_p-m_f-1)} & \\color{darkred}{\\cdots} & \\color{darkblue}{\\cdots} & \\color{darkblue}{\\cdots} & \\color{darkblue}{y(T)}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "you can see each row as independent instances, in which the red columns are features and the blue ones are targets.\n",
    "\n",
    "The forecasting task is now turned into a regression problem, in which we train the paramters $\\beta$ of a regressor $f_{\\beta}$:\n",
    "$$ \\min_{\\beta} \\Vert f_{\\beta}(Y_p) -Y_f \\Vert_2$$\n",
    "\n",
    "When the regressor $f$ is a linear model, the optimal values of $\\beta$ can be written as before:\n",
    "$$\\beta^* = (Y_p^TY_p)^{-1}(Y_p^TY_f)$$\n",
    "\n",
    "\n",
    "# ❓ Define an embedding function, returning $E$.\n",
    "The function takes as input the pd.Series s and the maximum number of lags m. Create m lags and stack them together to create teh embedding matrix $E$. Beware! We'll introduce some NaNs in the process, you can drop rows with nans using `dropna`\n"
   ],
   "metadata": {
    "id": "z6Z0v4SK7s6I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def embedding(s, m):\n",
    "  e = #?\n",
    "  return e\n",
    "\n",
    "embedding(df_power['power'], 10)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "TUWPa_iRSmzd",
    "outputId": "e6eb94a0-a7dd-4532-873e-b8d4957ed09e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ❓ Complete the linear direct forecaster\n",
    "The forecaster retrieve the embedding and perform a training, test split. Complete the fit procedure:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "m_p = 2\n",
    "m_f = 24\n",
    "\n",
    "def linear_forecaster(df, m_p, m_f):\n",
    "  e = embedding(df, m_p+m_f)\n",
    "  y_p_tr, y_f_tr = e.iloc[:-1, :m_p], e.iloc[:-1, m_p:m_f+m_p]\n",
    "  y_p_te, y_f_te = e.iloc[[-1], :m_p], e.iloc[[-1], m_p:m_f+m_p]\n",
    "\n",
    "  beta = None # ? COMPLETE THIS LINE\n",
    "  y_hat = y_p_te@beta.values\n",
    "  return y_f_te, y_hat\n",
    "\n",
    "y_f_te, y_hat = linear_forecaster(df_power['power'], m_p, m_f)\n",
    "# plot y_f_te and predictions\n",
    "plt.subplots(1, 1, figsize=(20, 4))\n",
    "plt.plot(df_power['power'].iloc[-24*8:-24].values.ravel(), label='ground truth')\n",
    "plt.plot(range(24*7, 24*7 + m_f), y_f_te.values.ravel(), label='ground truth')\n",
    "plt.plot(range(24*7, 24*7 + m_f), y_hat.values.ravel(), label='prediction')\n",
    "plt.legend();\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "O0V8QRQ7UWEE",
    "outputId": "771b64f8-af43-4ad3-d235-f86ab253a55f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def regression_interactive(df, m_p = 24, m_f = 24 * 7, plot_steps=24*8):\n",
    "  # Create dropdown to toggle order\n",
    "  k1_slider = widgets.IntSlider(value=2, min=2, max=1000, step=1, description='m_p', continuous_update=False)\n",
    "  k2_slider = widgets.IntSlider(value=24, min=3, max=1000, step=1, description='m_f', continuous_update=False)\n",
    "\n",
    "\n",
    "  def update_plot(m_p, m_f):\n",
    "    _, y_hat = linear_forecaster(df, m_p, m_f)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 4))\n",
    "    plt.plot(df.tail(plot_steps).values.ravel(), label='ground truth')\n",
    "    plt.plot(range(plot_steps-m_f, plot_steps), y_hat.values.ravel(), label='prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "  # Link widget to function\n",
    "  ui = widgets.VBox([k1_slider, k2_slider])\n",
    "  out = widgets.interactive_output(update_plot, {'m_p': k1_slider, 'm_f': k2_slider})\n",
    "\n",
    "  return display(ui, out)\n",
    "\n",
    "regression_interactive(df_power['power'])"
   ],
   "metadata": {
    "id": "G2KK6LMBluyc"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression via embeddings - recursive approach\n",
    "With Nixtla it's (only) possible to implement a recursive forecasting strategy.\n",
    "<img src=\"https://github.com/nepslor/B5203E-TSAF/raw/95ab6491476169ca761b47d1bee8735b12346694/pics/recursive_forecast.png\" width=\"600\">"
   ],
   "metadata": {
    "id": "ggajsOnnffCS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "fcst = MLForecast(models=[LinearRegression()],\n",
    "                   freq='1h',\n",
    "                   lags=[1])\n",
    "\n",
    "fcst.fit(df_nixtla.iloc[-24*30:-24])\n",
    "predictions = fcst.predict(h=24)\n",
    "plot_series(df_nixtla.tail(24*8), predictions)"
   ],
   "metadata": {
    "id": "avyVlyR7u9ja",
    "outputId": "2d395c8e-328c-4bbd-cc7f-08e374b6b99b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# we define a set of 5 forecasts, each of which is a linear model using\n",
    "# increasing lags of the target as features\n",
    "fcst = [MLForecast(models=[LinearRegression()],\n",
    "                   freq='1h',\n",
    "                   lags=list(range(1, i)))\n",
    "        for i in [2, 12, 24, 24*7]]\n",
    "\n",
    "[f.fit(df_nixtla.iloc[-24*30:-24]) for f in fcst];\n",
    "predictions = [f.predict(h=24) for f in fcst]\n",
    "\n",
    "# we stack the predictions together renaming them\n",
    "joined_predictions = []\n",
    "\n",
    "for i, prediction_df in enumerate(predictions):\n",
    "    prediction_df = prediction_df.rename(columns={'LinearRegression': f'LinearRegression_{i}'})\n",
    "    if i == 0:\n",
    "        joined_predictions = prediction_df\n",
    "    else:\n",
    "        joined_predictions = pd.merge(joined_predictions, prediction_df, on=['unique_id', 'ds'], how='outer')\n",
    "\n",
    "plot_series(df_nixtla.tail(24*8), joined_predictions)"
   ],
   "metadata": {
    "id": "JMgBAWAIvepo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "outputId": "fb1ed233-e368-4970-ab8e-460777dc1506"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pitfalls of Fourier forecast\n",
    "Fourier forecasting isn't a great choice when we are dealing with nonstationary, non periodic data.\n",
    "The following ECG data is a good example. These timeseries show repeating patterns but are not exactly cyclical."
   ],
   "metadata": {
    "id": "LEElKDb0ssWw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_ecg = pd.read_pickle('https://github.com/nepslor/teaching/raw/refs/heads/main/TimeSeriesForecasting/data/ecg_dat.pkl')"
   ],
   "metadata": {
    "id": "sNoXO9EZe4wX"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll try to forecast the first signal"
   ],
   "metadata": {
    "id": "h6lIpIWP94ja"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df_ecg.iloc[:, :5].plot(subplots=True, figsize=(20, 6), linewidth=0.3);"
   ],
   "metadata": {
    "id": "Qd-ev5D87N5p",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "outputId": "d461b5cd-b88c-4b39-8e2b-0b2c85e692aa"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_ecg.index = pd.date_range(start='2024-01-01', freq='1h', periods=len(df_ecg))"
   ],
   "metadata": {
    "id": "ag6eYGBXsJDR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_nixtla = convert_to_nixtla_ts(df_ecg.iloc[:, 0].astype(float))\n",
    "fourier_interactive(df_nixtla, 400, 1200)"
   ],
   "metadata": {
    "id": "OKqk0yxVgCFD"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "regression_interactive(df_ecg.iloc[:, 0].astype(float), 1000, 1000, 1200)"
   ],
   "metadata": {
    "id": "fMVK_-dBpfKJ"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
