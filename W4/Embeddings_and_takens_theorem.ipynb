{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nepslor/B5203E-TSAF/blob/main/W4/Embeddings_and_takens_theorem.ipynb)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Embeddings as forecasting features - Takens' Theorem\n",
    "Let's consider a (possibly nonlinear) dynamical, $n$-state system, which we write of the form:\n",
    "\n",
    "\\begin{equation}\n",
    "s_t=g(s_{t−1}) \\qquad (1)\n",
    "\\end{equation}\n",
    "\n",
    "For example, we can use the Lorenz system:\n",
    "The Lorenz system is an $n=3$ dynamical system defined as:\n",
    "\\begin{cases}\n",
    "\\begin{aligned}\n",
    "\\frac{dx}{dt} &= \\sigma (y - x), \\\\\n",
    "\\frac{dy}{dt} &= x (\\rho - z) - y, \\\\\n",
    "\\frac{dz}{dt} &= xy - \\beta z,\n",
    "\\end{aligned}\n",
    "\\end{cases}\n",
    "where $\\sigma$, $\\rho$, and $\\beta$ are parameters."
   ],
   "metadata": {
    "id": "6Ry_rvVvc_Rr"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "F_xtQngWsbDA"
   },
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the Lorenz system of differential equations\n",
    "def lorenz(t, y, sigma, rho, beta):\n",
    "    dydt = [sigma * (y[1] - y[0]),\n",
    "            y[0] * (rho - y[2]) - y[1],\n",
    "            y[0] * y[1] - beta * y[2]]\n",
    "    return dydt\n",
    "\n",
    "# Set the parameters\n",
    "sigma = 10.0\n",
    "rho = 28.0\n",
    "beta = 8.0 / 3.0\n",
    "\n",
    "# Set the initial conditions\n",
    "initial_conditions = [1.0, 0.0, 20.0]\n",
    "\n",
    "# Set the time span for integration\n",
    "t_span = (0, 120)\n",
    "t_eval = np.linspace(t_span[0], t_span[1], 12000)\n",
    "\n",
    "# Solve the system of differential equations\n",
    "sol = solve_ivp(lorenz, t_span, initial_conditions, args=(sigma, rho, beta), t_eval=t_eval)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the Lorenz attractor\n",
    "fig = plt.figure(figsize=(18, 4), layout='tight')\n",
    "ax = fig.add_subplot(141, projection='3d')\n",
    "ax.plot(sol.y[0], sol.y[1], sol.y[2], lw=0.5)\n",
    "ax.set_xlabel('X-axis')\n",
    "ax.set_ylabel('Y-axis')\n",
    "ax.set_zlabel('Z-axis')\n",
    "ax.set_title('Lorenz Attractor')\n",
    "\n",
    "# plot the first coordinate only\n",
    "ax = fig.add_subplot(1, 4, (2, 4))\n",
    "ax.plot(sol.t, sol.y[0], label='X-coordinate')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('X-coordinate')\n",
    "ax.set_title('Lorenz Attractor: X-coordinate vs Time')\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "ENwOZtM4sbDD",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "outputId": "2b3e38d8-8473-4760-9116-cc3ea36d99c8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Takens' theorem\n",
    "Since (1) is a deterministic system, the initial condition $s_{t0}$ determines the entire evolution of the system and, consequently, the entire realization of the observable. This means that the entire sequence depends on the initial condition, in the sense that different initial conditions will lead to different state sequences.\n",
    "\n",
    "We now wonder whether the opposite is true or not, i.e., if one can reconstruct the state of the system by observing a series of one-dimensional measures.\n",
    "Takens’ theorem deals with this question.\n",
    "$$e_t(m ,\\tau) = [x(t), x(t-\\tau),        \\cdots  x(t-(m-1)\\tau)]$$\n",
    "\n",
    "Takens has shown that embeddings with $m>2n$ will be faithful generically so that there is a smooth map $f:\\mathbb{R}^m→ \\mathbb{R}$ such that\n",
    "$$x_{t+1} = f(e_t(m, \\tau))$$\n",
    "\n",
    "We can define a dataset of delayed vectors\n",
    "$$E(m,\\tau) =\n",
    "\\begin{bmatrix}\n",
    "x(T)         & x(T-\\tau)       & \\cdots & x(T-(m-1)\\tau) \\\\\n",
    "x(T-1)       & x(T-\\tau-1)     & \\cdots & x(T-(m-1)\\tau-1) \\\\\n",
    "x(T-2)       & x(T-\\tau-2)     & \\cdots & x(T-(m-1)\\tau-2) \\\\\n",
    "\\vdots      & \\vdots      & \\ddots & \\vdots \\\\\n",
    "x((m-1)\\tau) & x((m-1)\\tau-\\tau) & \\cdots & x(0)\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "e_T(m, \\tau) \\\\\n",
    "e_{T-1}(m, \\tau)\\\\\n",
    "\\vdots\\\\\n",
    "e_{m\\tau}(m, \\tau)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and train a regressor to predict\n",
    "$$\\Vert f(E(m,\\tau)) -x \\Vert_2$$"
   ],
   "metadata": {
    "id": "keQzEOrHkNFy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the x-coordinate\n",
    "x = sol.y[0]\n",
    "\n",
    "#create train and test, discarding some initial points\n",
    "n_tr = 10000\n",
    "x_train, x_test = x[500:500+n_tr], x[n_tr+500:]\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YcHEhQ-LsbDE"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now create the function producing the embedding matrix $E(m,\\tau)$"
   ],
   "metadata": {
    "id": "PQayjg8M_Y3M"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a delay embedding\n",
    "def delay_embedding(data, d, tau):\n",
    "    N = len(data)\n",
    "    indices = np.arange(d) * tau + np.arange(N - (d - 1) * tau)[:, None]\n",
    "    embedded_data = data[indices]\n",
    "    return embedded_data"
   ],
   "metadata": {
    "id": "eXjsdileimlH"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "jOvy7HZZr0gq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Here's an example\n",
    "arr = np.array([1,2,3,4,5,6,7,8,9])\n",
    "emb = delay_embedding(arr,4,2)\n",
    "emb"
   ],
   "metadata": {
    "id": "2jTuTdwPpNKz",
    "outputId": "2434422d-581d-4c61-a5f1-2ae1ec96b12b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Set parameters for delay embedding\n",
    "d = 20  # Number of delays\n",
    "tau = 1  # Time delay\n",
    "\n",
    "# Create delay embedding\n",
    "embedded_data = delay_embedding(x_train, d, tau)\n",
    "plt.matshow(embedded_data[:500, :],aspect=0.04);\n",
    "plt.xlabel('First {} rows of E'.format(500));\n"
   ],
   "metadata": {
    "id": "0RIPWBBQicAS",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "outputId": "e7886477-0606-43e1-9115-0b4ea8dfbe1e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now try to approximate $f$ using a linear function.\n",
    "\n",
    "### ❓ Use Ridge regression to learn a linear model predicting one step ahead.\n"
   ],
   "metadata": {
    "id": "xGsVZb64uC5E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "X = #?\n",
    "Y = #?\n",
    "\n",
    "linear_predictor = #?\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pFy0VW8AsbDG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_reconstructed = linear_predictor.predict(X)\n",
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.plot(np.arange(len(Y))*1e-2,train_reconstructed,\"r\", label = \"Prediction\")\n",
    "plt.plot(np.arange(len(Y))*1e-2,Y, \"k--\", label = \"True\")\n",
    "plt.title(\"Training Set - Linear Model\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Train_linear\")\n",
    "plt.plot()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "X9F-lGeHsbDH",
    "outputId": "86744a98-ede2-4588-9397-eb13161fefa9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We did quite a good job at predicting one step ahead, but to really assess the goodness of the model we should try to forecast more than one step ahead. Let's implement a recursive strategy:\n",
    "\n",
    "<img src=\"https://github.com/nepslor/B5203E-TSAF/raw/95ab6491476169ca761b47d1bee8735b12346694/pics/recursive_forecast.png\" width=\"600\">"
   ],
   "metadata": {
    "id": "4IIrnTb7uSw6"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### ❓ Implement the recursive strategy and plot the results"
  },
  {
   "cell_type": "code",
   "source": [
    "x_start=X[-1,:]\n",
    "x_start = x_start[ np.newaxis,:]\n",
    "L_seq = 900\n",
    "x = x_start\n",
    "\n",
    "# recursive strategy:\n",
    "preds = []\n",
    "for _ in range(L_seq):\n",
    "    # ??\n",
    "    pass\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "p90QwyL1sbDI"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "preds = np.array(preds)\n",
    "final_part = train_reconstructed[-100:].reshape(-1,1)\n",
    "continuation = np.concatenate([final_part,preds[1:]])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.arange(len(continuation))*1e-2, continuation, 'r', label = \"Prediction\")\n",
    "plt.plot(np.arange(len(continuation))*1e-2,np.concatenate([x_train[-100:],x_test[:L_seq-1]]), 'k--', label = \"True\")\n",
    "\n",
    "plt.xlim([-0.1, (len(continuation)*1e-2) + 0.1])\n",
    "# Plot a horizontal line to separate train and prediction\n",
    "plt.axvline(x=len(final_part)*1e-2, color='g', linestyle='--', label='End of training')\n",
    "plt.title(\"Linear Model - Prediction\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Test_linear\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "9Xn8O8jZk0UY",
    "outputId": "f94bb9ca-b914-40bb-c97b-2a37f28a85f7"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We didn't go very far in predicting the system's trajectory. We can increase the expressivity of the model by trying to use a feed-forward neural net."
   ],
   "metadata": {
    "id": "Wsqbp3zRJVMz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# Create a simple neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 128\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# Create the neural network model\n",
    "model = NeuralNetwork(input_dim, hidden_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Lists to store loss history\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Append the current loss to the loss history list\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "# Use the trained model for prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor).numpy()\n",
    "\n",
    "# Plot the loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss History')\n",
    "plt.savefig('Loss')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "outputId": "8e0be10e-a6b8-40a2-f3a2-45fbd12c04a7",
    "id": "AfgclcgJsgt4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.plot(np.arange(len(Y))*1e-2,predictions,'r', label = \"Prediction\")\n",
    "plt.plot(np.arange(len(Y))*1e-2, Y, 'k--', label = \"True\")\n",
    "plt.title(\"Training Set - Neural Network\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Train_nn\")\n",
    "plt.plot()\n"
   ],
   "metadata": {
    "id": "eVqIpGFAzHdX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "90577677-5401-490f-89f1-d44ff5feb9a4"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "x_start=X[-1,:]\n",
    "x_start = x_start[ np.newaxis,:]\n",
    "x_start=torch.tensor(x_start, dtype=torch.float32)\n",
    "L_seq = 1500\n",
    "x = x_start\n",
    "\n",
    "nn_preds = []\n",
    "for _ in range(L_seq):\n",
    "    pred = model(x)\n",
    "    nn_preds.append(pred.detach().numpy()[0])\n",
    "    x = torch.concatenate((x[:,1:],pred), axis=1)\n",
    "\n"
   ],
   "metadata": {
    "id": "--2VoIh8zPeo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "nn_preds = np.array(nn_preds)\n",
    "final_part = train_reconstructed[-100:].reshape(-1,1)\n",
    "continuation_nonlin = np.concatenate([final_part,nn_preds[1:]])\n",
    "\n",
    "\n",
    "# FIGURE\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plt.plot(np.arange(len(continuation_nonlin))*1e-2, continuation_nonlin, 'r', label = \"Prediction\")\n",
    "plt.plot(np.arange(len(continuation_nonlin))*1e-2,np.concatenate([x_train[-100:],x_test[:L_seq-1]]), 'k--', label = \"True\")\n",
    "plt.xlim([-0.1, (len(continuation)*1e-2) + 0.1])\n",
    "# Plot a horizontal line to separate train and prediction\n",
    "plt.axvline(x=len(final_part)*1e-2, color='g', linestyle='--', label='End of training')\n",
    "\n",
    "plt.title(\"Neural Network - Prediction\")\n",
    "plt.legend()\n",
    "plt.savefig(\"Test_nn\")\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "id": "XdrdX8AS2AQ4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "outputId": "3c2c75cc-c88a-484e-ef8e-eee31b0d1fe9"
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
