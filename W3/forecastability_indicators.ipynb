{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nepslor/B5203E-TSAF/blob/main/W3/forecastability_indicators.ipynb)",
   "id": "3ae90b694d8fc9e3"
  },
  {
   "cell_type": "markdown",
   "id": "ecdc2acc",
   "metadata": {},
   "source": [
    "### Forecastability KPIs — definitions & interpretation\n",
    "In this notebook we'll characterize some time series from the M4 weekly dataset, containing series with different level of forecastability. Additionally to the a-priori forecastability indexes we introduced during the lecture, we'll see some more. The complete list is the following:\n",
    "| KPI | Formula | Meaning | More forecastable when… | Range |\n",
    "|---|---|---|---|---|\n",
    "| **Variance ratio** | $ \\dfrac{\\operatorname{Var}(\\Delta y_t)}{\\operatorname{Var}(y_t)} $, with $ \\Delta y_t = y_t - y_{t-1} $ | Roughness vs overall scale (higher ⇒ rougher) | **Lower** | $[0,\\infty)$ |\n",
    "| **Spectral entropy (normalized)** | $ H_{\\text{spec}} = -\\dfrac{1}{\\log K}\\sum_{i=1}^{K} p_i \\log p_i,\\ \\ p_i=\\dfrac{P_i}{\\sum_j P_j} $ | How concentrated the spectrum is (periodicity) | **Lower** | $[0,1]$ |\n",
    "| **Spectral forecastability** $\\,\\Omega$ | $ \\Omega = 1 - H_{\\text{spec}} $ | Complement of spectral entropy | **Higher** | $[0,1]$ |\n",
    "| **SVD entropy (normalized)** | $ H_{\\text{SVD}} = -\\dfrac{1}{\\log m}\\sum_{i=1}^{m} p_i \\log p_i,\\ \\ p_i=\\dfrac{\\sigma_i}{\\sum_j \\sigma_j} $ | Rank-like complexity of delay embedding | **Lower** | $[0,1]$ |\n",
    "| **Permutation entropy** ($m{=}5,\\ \\tau{=}1$) | $ H_{\\text{perm}} = -\\dfrac{1}{\\log(m!)} \\sum_{\\pi} p(\\pi)\\log p(\\pi) $ | Ordinal-pattern unpredictability (amp-invariant) | **Lower** | $[0,1]$ |\n",
    "| **Sample entropy** ($m{=}2,\\ r{=}\\,0.2\\,\\sigma$) | $ \\mathrm{SampEn}(m,r) = -\\ln\\!\\left(\\dfrac{A}{B}\\right) $ | Similar patterns remain similar one step ahead | **Lower** | $[0,\\infty)$ (often $0$–$3$) |\n",
    "| **Approximate entropy** ($m{=}2,\\ r{=}\\,0.2\\,\\sigma$) | $ \\operatorname{ApEn}(m,r) = \\Phi_m(r) - \\Phi_{m+1}(r) $ | Older entropy (counts self-matches); regularity | **Lower** | $[0,\\infty)$ (often $0$–$3$) |\n",
    "| **Lempel–Ziv complexity** (8-quantile symbols) | $ C_{\\text{LZ}}^{\\text{norm}} $ (normalized LZ76 parsing count) | Algorithmic randomness of discretized series | **Lower** | $[0,1]$ |\n",
    "| **Permutation entropy (ordpy)** | same as above, via ordinal distribution | Entropy on the complexity–entropy plane | **Lower** | $[0,1]$ |\n",
    "| **Statistical complexity (JS)** | $ C_{\\text{JS}} $ (Jensen–Shannon–based complexity from ordinal $p$ vs uniform) | Structured-but-nonrandomness; peaks at intermediate order | **Non-monotone** (descriptive) | typically $[0,1]$ |\n",
    "| **Autocorr. at lag 1** | $ \\rho(1) $ | Linear dependence at 1 step | **Higher in magnitude** ($|\\rho(1)|$) | $[-1,1]$ |\n",
    "| **Autocorr. strength (up to 52)** | $ \\sum_{k=1}^{52} |\\rho(k)| $ | Total linear structure up to yearly (weekly data) | **Higher** | $[0,52]$ |\n",
    "| **Periodicity index** | $ \\dfrac{\\max_{f>0} P(f)}{\\sum_{f>0} P(f)} $ | Share of power at the dominant non-DC frequency | **Higher** | $(0,1]$ |\n",
    "\n",
    "### Notes.\n",
    "For composites, invert the “lower-is-better” metrics, use $|\\rho(1)|$ for directionless strength, and consider z-scoring before aggregation. Many entropy/complexity measures prefer z-normalized inputs and enough length.\n",
    "\n",
    "\n",
    "### Preprocessing tips\n",
    "Z-normalize; handle missing values; for strongly seasonal data, compute metrics on both raw and seasonally adjusted series. For a monotone composite score, invert all “lower-is-better” metrics, use $|\\mathrm{ACF}(1)|$, and optionally transform DFA $\\alpha$ to $|\\alpha-0.5|$ before z-scoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f0a9badd",
   "metadata": {},
   "source": [
    "\n",
    "# Install dependencies\n",
    "%pip -q install pandas numpy antropy ordpy nolds statsmodels scipy joblib tqdm requests umap-learn anywidget ipywidgets plotly\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "95aa1c35",
   "metadata": {},
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "import antropy as ant\n",
    "import ordpy\n",
    "import nolds\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "DATA_DIR = Path('data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "RESULTS_DIR = Path('results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Toggle for speed: set to True to skip heavier metrics if needed\n",
    "FAST_MODE = False\n",
    "N_JOBS = -1  # Use all cores\n",
    "MAX_LAG_ACF = 52  # weekly: capture ~annual seasonality\n",
    "PERM_ORDER = 5\n",
    "SAMPEN_M = 2\n",
    "APEN_M = 2\n",
    "R_FRACTION = 0.2  # r = 0.2 * std\n",
    "LZC_BINS = 8\n",
    "RANDOM_SEED = 1337\n",
    "np.random.seed(RANDOM_SEED)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e330ac3",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Download M4 Weekly dataset\n",
    "\n",
    "Primary source: **Zenodo 4656410** (Monash TSF Repository).  \n",
    "We look for `M4_weekly_dataset.tsf`. If unavailable, we fall back to M4 GitHub `Weekly-train.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1555968f",
   "metadata": {},
   "source": [
    "\n",
    "ZENODO_RECORD = \"https://zenodo.org/api/records/4656410\"\n",
    "TARGET_TSF_NAME = \"M4_weekly_dataset.tsf\"\n",
    "FALLBACK_CSV_URL = \"https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/Train/Weekly-train.csv\"\n",
    "\n",
    "def download_zenodo_tsf(record_url: str, target_name: str, out_path: Path) -> bool:\n",
    "    try:\n",
    "        r = requests.get(record_url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        meta = r.json()\n",
    "        files = meta.get(\"files\", [])\n",
    "        for f in files:\n",
    "            if f.get(\"key\",\"\").endswith(target_name):\n",
    "                file_url = f[\"links\"][\"self\"]\n",
    "                print(f\"Downloading from Zenodo: {file_url}\")\n",
    "                fr = requests.get(file_url, timeout=60)\n",
    "                fr.raise_for_status()\n",
    "                out_path.write_bytes(fr.content)\n",
    "                print(f\"Saved TSF to {out_path}\")\n",
    "                return True\n",
    "        print(\"TSF file not found in Zenodo record files.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Zenodo download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def download_fallback_csv(url: str, out_path: Path) -> bool:\n",
    "    try:\n",
    "        print(f\"Downloading fallback CSV: {url}\")\n",
    "        r = requests.get(url, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        out_path.write_bytes(r.content)\n",
    "        print(f\"Saved CSV to {out_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Fallback CSV download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "tsf_path = DATA_DIR / TARGET_TSF_NAME\n",
    "csv_path = DATA_DIR / \"Weekly-train.csv\"\n",
    "\n",
    "HAVE_TSF = False\n",
    "if not tsf_path.exists():\n",
    "    HAVE_TSF = download_zenodo_tsf(ZENODO_RECORD, TARGET_TSF_NAME, tsf_path)\n",
    "else:\n",
    "    HAVE_TSF = True\n",
    "\n",
    "if not HAVE_TSF and not csv_path.exists():\n",
    "    _ = download_fallback_csv(FALLBACK_CSV_URL, csv_path)\n",
    "\n",
    "print(\"Files present:\", list(DATA_DIR.iterdir()))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "919ebe3c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Load dataset (TSF or CSV)\n",
    "\n",
    "- **TSF** (`M4_weekly_dataset.tsf`) — Monash TSF format.\n",
    "- **CSV** (`Weekly-train.csv`) — official M4 layout (each row is a series; first column = ID).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd04dbf9",
   "metadata": {},
   "source": [
    "\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def parse_tsf(path: Path) -> Tuple[List[str], List[np.ndarray]]:\n",
    "    ids, series = [], []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # Find @data\n",
    "    data_idx = None\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip().lower() == '@data':\n",
    "            data_idx = i + 1\n",
    "            break\n",
    "    if data_idx is None:\n",
    "        raise ValueError(\"Invalid TSF: missing @data section\")\n",
    "    for line in lines[data_idx:]:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\"):\n",
    "            continue\n",
    "        parts = [p.strip() for p in re.split(r',(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)', line)]\n",
    "        kv = {}\n",
    "        for p in parts:\n",
    "            if ':' in p:\n",
    "                k, v = p.split(':', 1)\n",
    "                kv[k.strip()] = v.strip().strip('\"')\n",
    "        sid = kv.get('series_name') or kv.get('series_id') or kv.get('series')\n",
    "        val = kv.get('series_value') or kv.get('series_values') or \"\"\n",
    "        if sid is None or val == \"\":\n",
    "            toks = line.split(',')\n",
    "            if len(toks) >= 2:\n",
    "                sid = sid or toks[0].split(':')[-1].strip()\n",
    "                val = val or toks[-1].split(':')[-1].strip()\n",
    "        val = val.replace('[','').replace(']','')\n",
    "        if ' ' in val and ',' not in val:\n",
    "            raw_vals = val.split()\n",
    "        else:\n",
    "            raw_vals = val.split(',')\n",
    "        vec = []\n",
    "        for s in raw_vals:\n",
    "            s = s.strip()\n",
    "            if s in {'?', 'NaN', 'nan', ''}:\n",
    "                vec.append(np.nan)\n",
    "            else:\n",
    "                try:\n",
    "                    vec.append(float(s))\n",
    "                except:\n",
    "                    pass\n",
    "        arr = np.asarray(vec, dtype=float)\n",
    "        if np.isnan(arr).any():\n",
    "            arr = arr[~np.isnan(arr)]\n",
    "        if arr.size == 0:\n",
    "            continue\n",
    "        ids.append(sid)\n",
    "        series.append(arr)\n",
    "    return ids, series\n",
    "\n",
    "def parse_m4_weekly_csv(path: Path) -> Tuple[List[str], List[np.ndarray]]:\n",
    "    df = pd.read_csv(path)\n",
    "    id_col = df.columns[0]\n",
    "    ids = df[id_col].astype(str).tolist()\n",
    "    vals = df.drop(columns=[id_col]).to_numpy()\n",
    "    out = []\n",
    "    for row in vals:\n",
    "        m = ~np.isnan(row)\n",
    "        if m.any():\n",
    "            out.append(row[m].astype(float))\n",
    "        else:\n",
    "            out.append(np.array([], dtype=float))\n",
    "    ids, out = zip(*[(i, x) for i, x in zip(ids, out) if x.size > 0])\n",
    "    return list(ids), list(out)\n",
    "\n",
    "if (DATA_DIR / \"M4_weekly_dataset.tsf\").exists():\n",
    "    print(\"Loading TSF...\")\n",
    "    series_ids, series_list = parse_tsf(DATA_DIR / \"M4_weekly_dataset.tsf\")\n",
    "elif (DATA_DIR / \"Weekly-train.csv\").exists():\n",
    "    print(\"Loading CSV...\")\n",
    "    series_ids, series_list = parse_m4_weekly_csv(DATA_DIR / \"Weekly-train.csv\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No dataset file available. Please ensure internet access and re-run the download cell.\")\n",
    "\n",
    "print(f\"Loaded {len(series_list)} series.\")\n",
    "print(\"Example lengths (first 5):\", [len(x) for x in series_list[:5]])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot some examples\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(3, 2, figsize=(12, 8))\n",
    "for i in range(6):\n",
    "    ax.ravel()[i].plot(series_list[i])\n",
    "    ax.ravel()[i].set_title(series_ids[i])\n",
    "plt.tight_layout()"
   ],
   "id": "6e12dcc661f326f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# discard series with less than 200 observations\n",
    "min_length = 260\n",
    "filtered = [(sid, x) for sid, x in zip(series_ids, series_list) if len(x) >= min_length]\n",
    "series_ids, series_list = zip(*filtered)"
   ],
   "id": "9eb76e2028137f30",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "377a4de9",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Metric functions\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "226f6fe8",
   "metadata": {},
   "source": [
    "\n",
    "from typing import Dict, Union, Optional\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "def variance_ratio(x: np.ndarray) -> Optional[float]:\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size < 3:\n",
    "        return np.nan\n",
    "    num = np.nanvar(np.diff(x))\n",
    "    den = np.nanvar(x)\n",
    "    return float(num / den) if den > 0 else np.nan\n",
    "\n",
    "def spectral_measures(x: np.ndarray) -> Dict[str, float]:\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size < 16:\n",
    "        return dict(spectral_entropy=np.nan, spectral_forecastability=np.nan, periodicity_index=np.nan)\n",
    "    try:\n",
    "        Hs = ant.spectral_entropy(x, sf=1.0, method='welch', normalize=True)\n",
    "    except Exception:\n",
    "        Hs = np.nan\n",
    "    Omega = np.nan if np.isnan(Hs) else 1.0 - Hs\n",
    "    try:\n",
    "        freqs, psd = signal.periodogram(x, detrend='linear', scaling='density')\n",
    "        if psd.size > 1:\n",
    "            peak = np.nanmax(psd[1:])\n",
    "            total = np.nansum(psd[1:])\n",
    "            per_idx = (peak / total) if total and total > 0 else np.nan\n",
    "        else:\n",
    "            per_idx = np.nan\n",
    "    except Exception:\n",
    "        per_idx = np.nan\n",
    "    return dict(spectral_entropy=Hs, spectral_forecastability=Omega, periodicity_index=per_idx)\n",
    "\n",
    "def svd_entropy_safe(x: np.ndarray, order: int = 3, delay: int = 1) -> float:\n",
    "    if x.size < order * delay + 1:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return ant.svd_entropy(x, order=order, delay=delay, normalize=True)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def perm_entropy_safe(x: np.ndarray, order: int = 5, delay: int = 1) -> float:\n",
    "    if x.size < order * delay + 1:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return ant.perm_entropy(x, order=order, delay=delay, normalize=True)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def sample_entropy_safe(x: np.ndarray, m: int = 2, r_fraction: float = 0.2) -> float:\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size < 20:\n",
    "        return np.nan\n",
    "    s = np.nanstd(x)\n",
    "    if s == 0:\n",
    "        return np.nan\n",
    "    for rf in (r_fraction, 0.25, 0.15, 0.3):\n",
    "        try:\n",
    "            val = ant.sample_entropy(x, order=m)\n",
    "            if np.isfinite(val):\n",
    "                return float(val)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return np.nan\n",
    "\n",
    "def approximate_entropy_safe(x: np.ndarray, m: int = 2, r_fraction: float = 0.2) -> float:\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size < 20:\n",
    "        return np.nan\n",
    "    s = np.nanstd(x)\n",
    "    if s == 0:\n",
    "        return np.nan\n",
    "    for rf in (r_fraction, 0.25, 0.15, 0.3):\n",
    "        try:\n",
    "            val = ant.app_entropy(x, order=m)\n",
    "            if np.isfinite(val):\n",
    "                return float(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.nan\n",
    "\n",
    "def quantize_series(x: np.ndarray, n_bins: int = 8) -> np.ndarray:\n",
    "    x = np.asarray(x, float)\n",
    "    if np.unique(x).size <= 1:\n",
    "        return np.zeros_like(x, dtype=int)\n",
    "    try:\n",
    "        q = pd.qcut(x, q=min(n_bins, max(2, np.unique(x).size)), labels=False, duplicates='drop')\n",
    "        return q.astype(int).to_numpy()\n",
    "    except Exception:\n",
    "        bins = np.linspace(np.nanmin(x), np.nanmax(x), n_bins + 1)\n",
    "        return np.digitize(x, bins[:-1], right=False).astype(int)\n",
    "\n",
    "def lzc_safe(x: np.ndarray, n_bins: int = 8) -> float:\n",
    "    if x.size < 16:\n",
    "        return np.nan\n",
    "    try:\n",
    "        sym = quantize_series(x, n_bins=n_bins)\n",
    "        return ant.lziv_complexity(sym, normalize=True)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def ordpy_complexity_entropy(x: np.ndarray, dx: int = 5, tau: int = 1) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      ordpy_perm_entropy: permutation entropy H (normalized)\n",
    "      ordpy_stat_complexity: Jensen–Shannon statistical complexity C (normalized)\n",
    "    Notes:\n",
    "      - Uses dx (embedding dimension along time).\n",
    "      - Uses taux=tau, tauy=1, dy=1 (1D patterns).\n",
    "      - Falls back to old API with `tau` if needed.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size < dx * tau + 1:\n",
    "        return dict(ordpy_perm_entropy=np.nan, ordpy_stat_complexity=np.nan)\n",
    "    try:\n",
    "        # Newer ordpy API\n",
    "        H, C = ordpy.complexity_entropy(x, dx=dx, dy=1, taux=tau, tauy=1, probs=False)\n",
    "    except Exception as e:\n",
    "        print(f\"ordpy error: {e}\")\n",
    "        # Older ordpy API fallback\n",
    "        H, C = ordpy.complexity_entropy(x, dx=dx, taux=tau)\n",
    "    return dict(ordpy_perm_entropy=float(H), ordpy_stat_complexity=float(C))\n",
    "\n",
    "def acf_measures(x: np.ndarray, max_lag: int = 52) -> Dict[str, float]:\n",
    "    if x.size < max_lag + 2:\n",
    "        return dict(acf1=np.nan, acf_mean_abs=np.nan)\n",
    "    try:\n",
    "        a = acf(x, nlags=max_lag, fft=True, missing='drop')\n",
    "        a1 = float(a[1]) if a.size > 1 else np.nan\n",
    "        asum = float(np.nanmean(np.abs(a[1:])))\n",
    "        return dict(acf1=a1, acf_mean_abs=asum)\n",
    "    except Exception:\n",
    "        return dict(acf1=np.nan, acf_mean_abs=np.nan)\n",
    "\n",
    "def dfa_alpha_safe(x: np.ndarray) -> float:\n",
    "    if x.size < 64:\n",
    "        return np.nan\n",
    "    try:\n",
    "        return float(nolds.dfa(x))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a6a3021c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Compute metrics for all series\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8bcc2c09",
   "metadata": {},
   "source": [
    "def compute_metrics_for_series(sid: str, x: np.ndarray) -> Dict[str, Union[str, float]]:\n",
    "    x = np.asarray(x, float)\n",
    "    x = x - np.nanmean(x)\n",
    "    s = np.nanstd(x)\n",
    "    if s > 0:\n",
    "        x = x / s\n",
    "    out = {\n",
    "        \"series_id\": sid,\n",
    "        \"length\": int(x.size),\n",
    "        \"var\": float(np.nanvar(x)) if x.size > 0 else np.nan,\n",
    "        \"variance_ratio\": variance_ratio(x),\n",
    "    }\n",
    "    out.update(spectral_measures(x))\n",
    "    out[\"svd_entropy\"] = svd_entropy_safe(x, order=3, delay=1)\n",
    "    out[\"perm_entropy\"] = perm_entropy_safe(x, order=PERM_ORDER, delay=1)\n",
    "    if FAST_MODE:\n",
    "        out[\"sample_entropy\"] = np.nan\n",
    "        out[\"approx_entropy\"] = np.nan\n",
    "        out[\"lzc\"] = np.nan\n",
    "        out[\"ordpy_perm_entropy\"] = np.nan\n",
    "        out[\"ordpy_stat_complexity\"] = np.nan\n",
    "    else:\n",
    "        out[\"sample_entropy\"] = sample_entropy_safe(x, m=SAMPEN_M, r_fraction=R_FRACTION)\n",
    "        out[\"approx_entropy\"] = approximate_entropy_safe(x, m=APEN_M, r_fraction=R_FRACTION)\n",
    "        out[\"lzc\"] = lzc_safe(x, n_bins=LZC_BINS)\n",
    "        oc = ordpy_complexity_entropy(x, dx=PERM_ORDER, tau=1)\n",
    "        out.update(oc)\n",
    "    out.update(acf_measures(x, max_lag=MAX_LAG_ACF))\n",
    "    out[\"dfa_alpha\"] = dfa_alpha_safe(x)\n",
    "    return out\n",
    "\n",
    "results = Parallel(n_jobs=N_JOBS, backend=\"loky\")(delayed(compute_metrics_for_series)(sid, x)\n",
    "                                                  for sid, x in tqdm(zip(series_ids, series_list), total=len(series_list)))\n",
    "\n",
    "\n",
    "df_metrics = pd.DataFrame(results)\n",
    "df_metrics.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4b3146f7",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Composite forecastability index\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "059d58ab",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "cols_invert = [\"spectral_entropy\",\"svd_entropy\",\"perm_entropy\",\"sample_entropy\",\"approx_entropy\",\n",
    "               \"ordpy_perm_entropy\",\"lzc\",\"variance_ratio\"]\n",
    "cols_keep = [\"spectral_forecastability\",\"periodicity_index\",\"acf1\",\"acf_mean_abs\",\n",
    "             \"ordpy_stat_complexity\",\"dfa_alpha\"]\n",
    "\n",
    "for c in cols_invert:\n",
    "    if c in df_metrics.columns:\n",
    "        df_metrics[c + \"_inv\"] = -df_metrics[c]\n",
    "\n",
    "use_cols = [c + \"_inv\" for c in cols_invert if c in df_metrics.columns] + [c for c in cols_keep if c in df_metrics.columns]\n",
    "\n",
    "df_comp = df_metrics[[\"series_id\"] + use_cols].copy()\n",
    "df_comp = df_comp.replace([np.inf,-np.inf], np.nan)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_comp[use_cols].to_numpy(dtype=float))\n",
    "comp_score = np.nanmean(X, axis=1)\n",
    "df_metrics[\"forecastability_index\"] = comp_score\n",
    "df_metrics[\"rank\"] = df_metrics[\"forecastability_index\"].rank(ascending=False, method=\"average\")\n",
    "\n",
    "df_metrics.sort_values(\"forecastability_index\", ascending=False).head(10)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot violinplots of metrics using seaborn\n",
    "import seaborn as sns\n",
    "melted = df_metrics.melt(id_vars=[\"series_id\"], value_vars=use_cols, var_name=\"metric\", value_name=\"value\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data=melted, x=\"metric\", y=\"value\", inner=\"quartile\", scale=\"width\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Distribution of Forecastability Metrics\")\n",
    "plt.tight_layout()"
   ],
   "id": "ad3c8bbdc0408347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot more and less forecastable series\n",
    "fig, ax = plt.subplots(len(use_cols), 2, figsize=(12, 30))\n",
    "for i, m in enumerate(use_cols):\n",
    "    idx_sorted = df_metrics[m].sort_values().index\n",
    "    best_idxs = idx_sorted[:4]\n",
    "    worst_idxs = idx_sorted[-4:]\n",
    "    for j in range(3):\n",
    "        ax[i, 0].plot(series_list[best_idxs[j]], linewidth=1)\n",
    "        ax[i, 1].plot(series_list[worst_idxs[j]], linewidth=1)\n",
    "    # set titles\n",
    "    ax[i, 0].set_title(f\"Best 3: {m}\")\n",
    "    ax[i, 1].set_title(f\"Worst 3: {m}\")\n",
    "plt.tight_layout()"
   ],
   "id": "5e8c30909bd6f1dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use UMAP to visualize forecastability space\n",
    "from umap.umap_ import UMAP\n",
    "reducer = UMAP(random_state=RANDOM_SEED)\n",
    "X_emb = reducer.fit_transform(df_comp[use_cols].to_numpy(dtype=float), y=df_metrics['forecastability_index'].to_numpy(dtype=float))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sc = plt.scatter(X_emb[:,0], X_emb[:,1], c=df_metrics[\"forecastability_index\"], cmap='viridis', s=10)\n",
    "plt.colorbar(sc, label='Forecastability Index')\n",
    "plt.title('UMAP Projection of Forecastability Metrics')\n",
    "plt.xlabel('UMAP 1')\n",
    "plt.ylabel('UMAP 2')\n",
    "plt.show()"
   ],
   "id": "8c96eaaa19a03ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Visualization via UMAP\n",
    "In the following cell we'll use UMAP to visualize the forecastability space defined by the indicators we computed.\n",
    "UMAP (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique based on manifold learning. As PCA, it can be used to project a multidimensional space into a lower dimensional one (usually 2D/3D for visualization).\n",
    "\n",
    "\n",
    "The target (or y) option in UMAP's fit_transform method allows you to provide additional information (such as labels or a continuous variable) to guide the embedding. When specified, UMAP can use this target information to influence the layout, potentially making the embedding more discriminative with respect to the target. This is especially useful for supervised or semi-supervised visualization tasks."
   ],
   "id": "10a1cc607d5e1863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except:\n",
    "    pass\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df_emb = df_metrics.copy()\n",
    "df_emb['umap_x'] = X_emb[:,0]\n",
    "df_emb['umap_y'] = X_emb[:,1]\n",
    "df_emb.head()\n",
    "\n",
    "id_to_series = {sid: s for sid, s in zip(series_ids, series_list)}\n",
    "if 'series_id' not in df_emb.columns:\n",
    "    raise KeyError(\"df_metrics must contain a 'series_id' column.\")\n",
    "default_color = 'forecastability_index' if 'forecastability_index' in df_emb.columns else use_cols[0]\n",
    "fig = make_subplots(rows=1, cols=2, column_widths=[0.58, 0.42],\n",
    "                    subplot_titles=('UMAP of Forecastability Metrics', 'Time Series Preview'))\n",
    "scatter = go.Scattergl(\n",
    "    x=df_emb['umap_x'],\n",
    "    y=df_emb['umap_y'],\n",
    "    mode='markers',\n",
    "    marker=dict(size=6, color=df_emb[default_color], colorscale='Viridis', showscale=True,\n",
    "                colorbar=dict(title=default_color)),\n",
    "    text=df_emb['series_id'],\n",
    "    hovertemplate='<b>%{text}</b><br>UMAP1=%{x:.3f}<br>UMAP2=%{y:.3f}<extra></extra>',\n",
    "    name='UMAP points'\n",
    ")\n",
    "fig.add_trace(scatter, row=1, col=1)\n",
    "line = go.Scatter(x=[], y=[], mode='lines', name='series')\n",
    "fig.add_trace(line, row=1, col=2)\n",
    "fig.update_xaxes(title_text='UMAP 1', row=1, col=1)\n",
    "fig.update_yaxes(title_text='UMAP 2', row=1, col=1)\n",
    "fig.update_xaxes(title_text='t (index)', row=1, col=2)\n",
    "fig.update_yaxes(title_text='value (z-norm)', row=1, col=2)\n",
    "fig.update_layout(height=600, margin=dict(l=10, r=10, t=40, b=10))\n",
    "figw = go.FigureWidget(fig)\n",
    "scatter_trace = figw.data[0]\n",
    "line_trace = figw.data[1]\n",
    "def znorm(x):\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size == 0: return x\n",
    "    m = np.nanmean(x); s = np.nanstd(x)\n",
    "    return (x - m) / s if s > 0 else x - m\n",
    "def hover_fn(trace, points, state):\n",
    "    if not points.point_inds: return\n",
    "    idx = points.point_inds[0]\n",
    "    sid = df_emb.iloc[idx]['series_id']\n",
    "    series = id_to_series.get(sid, None)\n",
    "    if series is None or len(series) == 0:\n",
    "        with figw.batch_update():\n",
    "            line_trace.x = []; line_trace.y = []\n",
    "            figw.layout.annotations[1].text = f'Time Series Preview (No data for {sid})'\n",
    "        return\n",
    "    x = np.arange(len(series)); y = znorm(series)\n",
    "    with figw.batch_update():\n",
    "        line_trace.x = x; line_trace.y = y; line_trace.name = str(sid)\n",
    "        figw.layout.annotations[1].text = f'Time Series Preview — {sid}'\n",
    "scatter_trace.on_hover(hover_fn)\n",
    "options = [c for c in df_emb.columns if c not in ['umap_x','umap_y'] and df_emb[c].dtype != 'O']\n",
    "if default_color not in options: options.insert(0, default_color)\n",
    "color_dd = widgets.Dropdown(options=options, value=default_color, description='Color by:', layout=widgets.Layout(width='350px'))\n",
    "def on_color_change(change):\n",
    "    col = change['new']\n",
    "    with figw.batch_update():\n",
    "        figw.data[0].marker.color = df_emb[col]\n",
    "        figw.data[0].marker.colorbar.title = col\n",
    "color_dd.observe(on_color_change, names='value')\n",
    "display(color_dd)\n",
    "display(figw)"
   ],
   "id": "883ed3658367970b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
