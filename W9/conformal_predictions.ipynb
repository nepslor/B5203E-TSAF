{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP9Mk8BrJFAK86OVGWcw/xb",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nepslor/B5203E-TSAF/blob/main/W9/conformal_predictions.ipynb)"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bootstrap vs conformal predictions\n",
    "In this exercise we will retrieve prediction intervals for a forecaster comparing bootstrap and conformal predictions. \n",
    "We will use the [ERA5-hourly](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview) reanalysis dataset from Copernicus. The dataset contains forecasts from a NWP model, updated ex-post with ground truth observations.  \n",
    "<center>\n",
    "<img src=https://datastore.copernicus-climate.eu/c3s/published-forms-v2/c3sprod/reanalysis-era5-single-levels/overview.jpg width=\"300\">\n",
    "</center>\n",
    "\n",
    "We retrieve the last 3 years of temperature data for a 5x5 square centered in Lugano. Since the resolution of the dataset is ~0.25 degree, the covered region is approximately the one in the figure.\n",
    "<center>\n",
    "<img src=https://raw.githubusercontent.com/nepslor/teaching/main/TimeSeriesForecasting/figs/copernicus.png\n",
    " width=\"300\", centered=True>\n",
    "</center>\n",
    "\n"
   ],
   "metadata": {
    "id": "OvA40tsaMFRq"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IXPlYrBZHKrM"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import matplotlib.animation as animation\n",
    "from lightgbm import LGBMRegressor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "all_dat = pd.read_pickle('https://github.com/nepslor/teaching/raw/main/TimeSeriesForecasting/data/copernicus/copernicus_dataset.zip')"
   ],
   "metadata": {
    "id": "SZ2cUGy0Im0d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#@title animation functions\n",
    "\n",
    "def scatter_animation(x, y, c, n_rows=50):\n",
    "    fig, ax = plt.subplots(1);\n",
    "\n",
    "    scat = ax.scatter(x, y, c=c[0, :], s=400);\n",
    "\n",
    "\n",
    "    def animate(i):\n",
    "        scat.set_array(c[i, :]);\n",
    "        return scat,\n",
    "\n",
    "    def init():\n",
    "        scat.set_array(c[0, :]);\n",
    "        return scat,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init, frames=n_rows, interval=100, blit=True)\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "def ts_animation(y_te, y_hat, n_rows=50):\n",
    "    fig, ax = plt.subplots(1);\n",
    "    t = np.arange(y_hat.shape[1])\n",
    "    line1, = ax.plot(y_hat[0, :], lw=2);\n",
    "    line2, = ax.plot(y_te[0, :], lw=2);\n",
    "    ax.set_ylim(-13, 13)\n",
    "    def animate(i):\n",
    "        line1.set_data(t, y_te[i, :]);\n",
    "        line2.set_data(t,y_hat[i, :]);\n",
    "        return (line1, line2,)\n",
    "\n",
    "    def init():\n",
    "        line1.set_data([], []);\n",
    "        return (line1,)\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, init_func=init, frames=n_rows, interval=100, blit=True)\n",
    "    plt.close('all')\n",
    "    return HTML(ani.to_jshtml())\n"
   ],
   "metadata": {
    "id": "t16ihJCEI5Q_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "all_dat['latlong'] = all_dat['lat'] + all_dat['long']*1000\n",
    "c_mat = all_dat.pivot(columns='latlong', index='time',values='T')\n",
    "scatter_animation(all_dat.loc[all_dat['time']==all_dat.loc[0, 'time'], 'lat'].values,\n",
    "                   all_dat.loc[all_dat['time']==all_dat.loc[0, 'time'] ,'long'].values, c=c_mat.values, n_rows=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "fDegAGHxJF2K",
    "outputId": "7c20ddd9-1ebe-4cb0-cd50-f052686cf56b"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "lagged_mav = lambda x, k: x.copy().rolling('{}d'.format(k)).mean()\n",
    "\n",
    "\n",
    "target_col = 8946.0\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 8))\n",
    "filt = c_mat.index>'2022-10'\n",
    "c_mat.loc[filt, [c for c in c_mat if c != target_col]].plot(alpha=0.1, ax=ax[0])\n",
    "c_mat.loc[filt, target_col].plot(ax=ax[0])\n",
    "\n",
    "c_mat_detr = pd.concat([c_mat[c]-lagged_mav(c_mat[c], 24) for c in c_mat.columns], axis=1)\n",
    "c_mat_detr.loc[filt, [c for c in c_mat if c != target_col]].plot(alpha=0.1, ax=ax[1])\n",
    "c_mat_detr.loc[filt, target_col].plot(ax=ax[1])\n",
    "[a.legend(fontsize='xx-small', ncols=8) for a in ax]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 691
    },
    "id": "-HQc2TW8KVIu",
    "outputId": "229e3f7c-4cef-4625-b6ef-735fd06d71bd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class LGBM:\n",
    "  \"\"\"\n",
    "  A simple regressor-based forecaster\n",
    "  \"\"\"\n",
    "  def __init__(self, pars):\n",
    "    self.pars = pars\n",
    "    self.target_cols = None\n",
    "    self.models = []\n",
    "\n",
    "  def fit(self, x, y):\n",
    "    self.target_cols = y.columns\n",
    "    for c in y.columns:\n",
    "        m = LGBMRegressor(**self.pars).fit(x.values, y[c].values.ravel())\n",
    "        self.models.append(m)\n",
    "    return self\n",
    "\n",
    "  def predict(self, x):\n",
    "    y_hat = []\n",
    "    for m, c in zip(self.models, self.target_cols):\n",
    "        y_hat.append(pd.Series(m.predict(x.values), index=x.index, name=c))\n",
    "    return pd.concat(y_hat, axis=1)\n"
   ],
   "metadata": {
    "id": "UJ9SirqlKsp8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def get_hankel(df, embedding=3):\n",
    "  return pd.concat([df.shift(-l).rename('{}_step_{}'.format(df.name, l)) for l in range(embedding)], axis=1).iloc[:-embedding]\n",
    "\n",
    "# create regressors\n",
    "n_sa = 24\n",
    "n_embedding = 26\n",
    "exogenous = [c for c in c_mat if c != target_col]\n",
    "x = pd.concat([get_hankel(c_mat_detr[e], embedding=n_embedding) for e in exogenous], axis=1)\n",
    "target_past = get_hankel(c_mat_detr[target_col], embedding=n_embedding + n_sa).iloc[:, :n_embedding]\n",
    "target_future = get_hankel(c_mat_detr[target_col], embedding=n_embedding + n_sa).iloc[:, n_embedding:]\n",
    "\n",
    "# create dataframes with just the target and the target plus exogenous variables\n",
    "df = pd.concat({'x':target_past, 'y': target_future}, axis=1)\n",
    "df_ex = pd.concat({'x':pd.concat([x, target_past], axis=1), 'y': target_future}, axis=1)\n"
   ],
   "metadata": {
    "id": "0O7WxxtOK3Wp"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# create training and test sets\n",
    "tr_ratio = 0.6\n",
    "cal_ratio = 0.2\n",
    "\n",
    "n_tr = int(df.shape[0] * tr_ratio)\n",
    "n_cal = int(df.shape[0] * cal_ratio)\n",
    "\n",
    "x_tr, x_cal, x_te = df['x'].iloc[:-n_tr], df['x'].iloc[n_tr:n_tr+n_cal], df['x'].iloc[-n_cal:]\n",
    "xex_tr, xex_cal, xex_te  = df_ex['x'].iloc[:-n_tr], df_ex['x'].iloc[n_tr:n_tr+n_cal], df_ex['x'].iloc[-n_cal:]\n",
    "y_tr, y_cal, y_te = df['y'].iloc[:-n_tr], df['y'].iloc[n_tr:n_tr+n_cal], df['y'].iloc[-n_cal:]\n",
    "yex_tr, yex_cal, yex_te = df_ex['y'].iloc[:-n_tr], df_ex['y'].iloc[n_tr:n_tr+n_cal], df_ex['y'].iloc[-n_cal:]\n"
   ],
   "metadata": {
    "id": "gs5mXq_YK5CN"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# train the models\n",
    "pars = {'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': 5}\n",
    "lgbm = LGBM(pars).fit(x_tr, y_tr)\n",
    "lgbm_ex = LGBM(pars).fit(xex_tr, yex_tr)\n",
    "y_hat = lgbm.predict(x_te)\n",
    "y_hat_ex = lgbm_ex.predict(xex_te)\n"
   ],
   "metadata": {
    "id": "j657durkK7Hs"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ts_animation(y_te.values, y_hat.values, n_rows=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "66LpHNGSL_hP",
    "outputId": "cfa502cc-277a-4536-eab1-2e476e2ea705"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "ts_animation(y_te.values, y_hat_ex.values, n_rows=100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "WLgl8KRYNfT_",
    "outputId": "e932af6d-63f7-4c94-8a36-f8ae11912b9d"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "(((y_te-y_hat)**2).mean()**0.5).plot(label='no exogenous')\n",
    "(((y_te-y_hat_ex)**2).mean()**0.5).plot(label='exogenous')\n",
    "plt.legend()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "UuCeidWrLILH",
    "outputId": "43db3290-2b79-45cb-d432-c36652e75b5f"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ❓Create confidence intervals through bootstrap\n",
    "Using the already obtained forecasts $\\hat{y}$\n",
    "1. retrieve bootstrapped prediction intervals for the forecasts. \n",
    "2. use the code we've seen in the last lesson to compute the CRPS and the reliability of the forecasts\n"
   ],
   "metadata": {
    "id": "guDuHxy4OLT1"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "GDn08fmoO20Z"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ❓ Create confidence intervals with conformal prediction\n",
    "Assuming that we use the absolute forecast error as non-conformity score\n",
    "1. obtain the conformal prediction intervals using the calibration set $\\mathcal{D}_{cal}$. \n",
    "2. Compute the CRSP and the reliabilty \n",
    "3. There should be an obvious trade off between the performances of bootstrap and the conformal prediction, as a function of the size of the training/calibration sets ratio: the bigger the calibration set, the more reliable the prediction intervals of CP. At the same time, the model will be trained on less data, producing worse prediction. Try to see if you can see this effect."
   ],
   "metadata": {
    "id": "oh3zqoHkO0AK"
   }
  }
 ]
}
